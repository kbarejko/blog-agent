#  Blog Agent - Alternatywne wersje dla r贸偶nych platform AI

G贸wna wersja (`blog_agent.py`) u偶ywa Anthropic Claude API. Poni偶ej znajdziesz instrukcje jak dostosowa agenta dla innych platform.

---

## 1. OpenAI (ChatGPT / GPT-4)

### Instalacja
```bash
pip install openai --break-system-packages
```

### Modyfikacje w kodzie

Zamie w pliku `blog_agent.py`:

```python
# ZAMIAST:
import anthropic
self.client = anthropic.Anthropic(api_key=self.api_key)
self.model = "claude-sonnet-4-20250514"

message = self.client.messages.create(
    model=self.model,
    max_tokens=4000,
    messages=[{"role": "user", "content": prompt}]
)
response_text = message.content[0].text

# U呕YJ:
from openai import OpenAI
self.client = OpenAI(api_key=self.api_key)
self.model = "gpt-4-turbo-preview"  # lub "gpt-4", "gpt-3.5-turbo"

response = self.client.chat.completions.create(
    model=self.model,
    max_tokens=4000,
    messages=[{"role": "user", "content": prompt}]
)
response_text = response.choices[0].message.content
```

### Zmienna rodowiskowa
```bash
export OPENAI_API_KEY='sk-...'
```

---

## 2. Google Gemini

### Instalacja
```bash
pip install google-generativeai --break-system-packages
```

### Modyfikacje w kodzie

```python
# ZAMIAST importu anthropic:
import google.generativeai as genai

class BlogAgent:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.environ.get("GOOGLE_API_KEY")
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel('gemini-pro')
    
    # W metodach gdzie wysyasz prompt:
    response = self.model.generate_content(prompt)
    response_text = response.text
```

### Zmienna rodowiskowa
```bash
export GOOGLE_API_KEY='AIza...'
```

---

## 3. Ollama (Lokalne modele)

### Instalacja Ollama
```bash
# Linux/Mac
curl -fsSL https://ollama.com/install.sh | sh

# cignij model
ollama pull llama3.1
ollama pull mistral
```

### Instalacja biblioteki Python
```bash
pip install ollama --break-system-packages
```

### Modyfikacje w kodzie

```python
# ZAMIAST importu anthropic:
import ollama

class BlogAgent:
    def __init__(self, model_name: str = "llama3.1"):
        self.model = model_name
    
    # W metodach gdzie wysyasz prompt:
    response = ollama.chat(
        model=self.model,
        messages=[{'role': 'user', 'content': prompt}]
    )
    response_text = response['message']['content']
```

### U偶ycie
```python
# Nie potrzebujesz klucza API!
agent = BlogAgent(model_name="llama3.1")
```

---

## 4. Cohere

### Instalacja
```bash
pip install cohere --break-system-packages
```

### Modyfikacje w kodzie

```python
# ZAMIAST importu anthropic:
import cohere

class BlogAgent:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.environ.get("COHERE_API_KEY")
        self.client = cohere.Client(self.api_key)
        self.model = "command"
    
    # W metodach gdzie wysyasz prompt:
    response = self.client.chat(
        message=prompt,
        model=self.model
    )
    response_text = response.text
```

### Zmienna rodowiskowa
```bash
export COHERE_API_KEY='...'
```

---

## 5. Azure OpenAI

### Instalacja
```bash
pip install openai --break-system-packages
```

### Modyfikacje w kodzie

```python
# ZAMIAST importu anthropic:
from openai import AzureOpenAI

class BlogAgent:
    def __init__(self):
        self.client = AzureOpenAI(
            api_key=os.environ.get("AZURE_OPENAI_KEY"),
            api_version="2023-05-15",
            azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT")
        )
        self.model = "gpt-4"  # Nazwa twojego deployment
    
    # W metodach gdzie wysyasz prompt:
    response = self.client.chat.completions.create(
        model=self.model,
        messages=[{"role": "user", "content": prompt}]
    )
    response_text = response.choices[0].message.content
```

### Zmienne rodowiskowe
```bash
export AZURE_OPENAI_KEY='...'
export AZURE_OPENAI_ENDPOINT='https://your-resource.openai.azure.com/'
```

---

## 6. Hugging Face (Inference API)

### Instalacja
```bash
pip install huggingface_hub --break-system-packages
```

### Modyfikacje w kodzie

```python
# ZAMIAST importu anthropic:
from huggingface_hub import InferenceClient

class BlogAgent:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.environ.get("HF_TOKEN")
        self.client = InferenceClient(token=self.api_key)
        self.model = "meta-llama/Llama-2-70b-chat-hf"
    
    # W metodach gdzie wysyasz prompt:
    response = self.client.text_generation(
        prompt=prompt,
        model=self.model,
        max_new_tokens=2000
    )
    response_text = response
```

### Zmienna rodowiskowa
```bash
export HF_TOKEN='hf_...'
```

---

## 7. Perplexity AI

### Instalacja
```bash
pip install openai --break-system-packages  # Perplexity u偶ywa OpenAI SDK
```

### Modyfikacje w kodzie

```python
# ZAMIAST importu anthropic:
from openai import OpenAI

class BlogAgent:
    def __init__(self, api_key: str = None):
        self.api_key = api_key or os.environ.get("PERPLEXITY_API_KEY")
        self.client = OpenAI(
            api_key=self.api_key,
            base_url="https://api.perplexity.ai"
        )
        self.model = "llama-3.1-sonar-large-128k-online"
    
    # W metodach gdzie wysyasz prompt:
    response = self.client.chat.completions.create(
        model=self.model,
        messages=[{"role": "user", "content": prompt}]
    )
    response_text = response.choices[0].message.content
```

### Zmienna rodowiskowa
```bash
export PERPLEXITY_API_KEY='pplx-...'
```

---

## 8. Bash z curl (bez Pythona!)

Mo偶esz u偶y blog agenta bezporednio z bash i curl. Oto przykad:

```bash
#!/bin/bash

# Konfiguracja
API_KEY="tw贸j-klucz-api"
TOPIC="Jak AI zmienia content marketing"

# Funkcja do wysyania promptu
call_api() {
    local prompt="$1"
    curl -s https://api.anthropic.com/v1/messages \
        -H "x-api-key: $API_KEY" \
        -H "anthropic-version: 2023-06-01" \
        -H "content-type: application/json" \
        -d "{
            \"model\": \"claude-sonnet-4-20250514\",
            \"max_tokens\": 4000,
            \"messages\": [{\"role\": \"user\", \"content\": \"$prompt\"}]
        }" | jq -r '.content[0].text'
}

# 1. Tworzenie konspektu
echo "Tworz konspekt..."
OUTLINE=$(call_api "Stw贸rz konspekt artykuu na temat: $TOPIC")
echo "$OUTLINE"

# 2. Pisanie sekcji (uproszczone)
echo "Pisz artyku..."
ARTICLE=$(call_api "Na podstawie tego konspektu napisz peny artyku: $OUTLINE")

# 3. Zapisanie
echo "$ARTICLE" > article.md
echo "Gotowe! Artyku zapisany w article.md"
```

---

## Por贸wnanie platform

| Platforma | Zalety | Wady | Koszt |
|-----------|---------|------|-------|
| **Claude (Anthropic)** | Najlepsza jako tekst贸w, wietne zrozumienie kontekstu | Wymaga API key | $$$ |
| **GPT-4 (OpenAI)** | Bardzo dobra jako, popularne | Drogie | $$$$ |
| **GPT-3.5 (OpenAI)** | Tanie, szybkie | rednia jako | $ |
| **Gemini (Google)** | Darmowy tier, dobre multimodalne | R贸偶na jako | $-$$ |
| **Ollama** | Cakowicie darmowe, prywatne | Wymaga mocy obliczeniowej | Gratis |
| **Mistral** | Dobry balans jakoci/ceny | Mniejsza dostpno | $$ |
| **Llama 3.1** | Open source, elastyczne | Trzeba hostowa | Gratis/$$$ |

---

## Zalecenia

**Dla najlepszej jakoci artyku贸w:**
1. Claude Sonnet 4 (u偶ywane domylnie)
2. GPT-4 Turbo
3. Claude Opus 4

**Dla balansu jako/cena:**
1. Claude Sonnet 4
2. GPT-3.5 Turbo
3. Mistral Large

**Dla prywatnoci/darmowe:**
1. Ollama + Llama 3.1
2. Local Mistral
3. Hugging Face (self-hosted)

---

## Szybka konwersja

Stworzyem gotowe warianty w katalogu `variants/`:

```bash
variants/
 blog_agent_openai.py      # Wersja dla OpenAI
 blog_agent_gemini.py      # Wersja dla Google Gemini
 blog_agent_ollama.py      # Wersja dla Ollama (lokalna)
 blog_agent.sh             # Wersja bash (curl)
```

---

**Potrzebujesz pomocy z konkretn platform?** 
Sprawd藕 dokumentacj danej platformy lub otw贸rz issue na GitHubie!
