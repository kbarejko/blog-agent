# AI Provider Configuration
# Configure AI providers for article generation

providers:
  # Claude (Anthropic) - Primary provider
  claude:
    api_key: ${ANTHROPIC_API_KEY}  # Set via environment variable
    model: claude-sonnet-4-20250514
    max_tokens: 4000
    temperature: 1.0

  # OpenAI - GPT-4 Turbo
  openai:
    api_key: ${OPENAI_API_KEY}  # Set via environment variable
    model: gpt-4-turbo
    max_tokens: 4000
    temperature: 0.7

  # OpenAI - GPT-4o (latest and most capable, recommended)
  openai-gpt4o:
    api_key: ${OPENAI_API_KEY}
    model: gpt-4o
    max_tokens: 4000
    temperature: 0.7

  # OpenAI - GPT-4o Mini (faster and cheaper)
  openai-gpt4o-mini:
    api_key: ${OPENAI_API_KEY}
    model: gpt-4o-mini
    max_tokens: 4000
    temperature: 0.7

  # OpenAI - GPT-3.5 Turbo (cheaper, legacy)
  openai-gpt35:
    api_key: ${OPENAI_API_KEY}
    model: gpt-3.5-turbo
    max_tokens: 4000
    temperature: 0.7

  # OpenAI - GPT-5 (latest flagship model, most capable)
  # Released: August 7, 2025
  #
  # GPT-5 Reasoning Model Notes:
  # - Only supports temperature=1.0 (custom temperature rejected by API)
  # - Uses internal reasoning tokens before output
  # - Token limits auto-adjusted in OpenAIProvider (e.g., 800 â†’ 8000)
  # - Actual API model name: gpt-5-2025-08-07
  # - Uses max_completion_tokens (not max_tokens)
  # - Supports up to 128K output tokens
  openai-gpt5:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-2025-08-07
    max_tokens: 32000
    temperature: 1.0

  # OpenAI - GPT-5 Mini (faster and cheaper than GPT-5)
  # Same reasoning model characteristics as GPT-5
  openai-gpt5-mini:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-mini-2025-08-07
    max_tokens: 16000
    temperature: 1.0

  # OpenAI - GPT-5 Nano (smallest and cheapest GPT-5 variant)
  # Same reasoning model characteristics as GPT-5
  openai-gpt5-nano:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-nano-2025-08-07
    max_tokens: 8000
    temperature: 1.0

  # Aliases for convenience (all naming variants)
  gpt5:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-2025-08-07
    max_tokens: 32000
    temperature: 1.0

  gpt-5:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-2025-08-07
    max_tokens: 32000
    temperature: 1.0

  gpt-5.1:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-2025-08-07
    max_tokens: 32000
    temperature: 1.0

  gpt5-mini:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-mini-2025-08-07
    max_tokens: 16000
    temperature: 1.0

  gpt-5-mini:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-mini-2025-08-07
    max_tokens: 16000
    temperature: 1.0

  gpt-5.1-mini:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-mini-2025-08-07
    max_tokens: 16000
    temperature: 1.0

  gpt5-nano:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-nano-2025-08-07
    max_tokens: 8000
    temperature: 1.0

  gpt-5-nano:
    api_key: ${OPENAI_API_KEY}
    model: gpt-5-nano-2025-08-07
    max_tokens: 8000
    temperature: 1.0

  # Google Gemini 2.5 Pro (recommended, most capable)
  gemini:
    api_key: ${GOOGLE_API_KEY}  # Set via environment variable
    model: gemini-2.5-pro
    max_tokens: 8000
    temperature: 0.9

  # Google Gemini 2.5 Flash (faster, cheaper)
  gemini-flash:
    api_key: ${GOOGLE_API_KEY}
    model: gemini-2.5-flash
    max_tokens: 8000
    temperature: 0.9

  # Google Gemini 2.0 Flash (legacy, but still supported)
  gemini-2.0-flash:
    api_key: ${GOOGLE_API_KEY}
    model: gemini-2.0-flash-001
    max_tokens: 8000
    temperature: 0.9

  # Google Gemini Pro (alias for gemini-2.5-pro for backwards compatibility)
  gemini-pro:
    api_key: ${GOOGLE_API_KEY}
    model: gemini-2.5-pro
    max_tokens: 8000
    temperature: 0.9

  # Ollama - Local models
  ollama:
    model: llama3:latest  # Change to any installed model (llama3, mistral, codellama, etc.)
    host: http://192.168.0.136:11434  # Ollama server URL (Windows host)
    max_tokens: 4000
    temperature: 0.7

  # Ollama - Mistral variant
  ollama-mistral:
    model: mistral:latest
    host: http://192.168.0.136:11434
    max_tokens: 4000
    temperature: 0.7

  # Ollama - CodeLlama variant
  ollama-codellama:
    model: codellama:13b
    host: http://192.168.0.136:11434
    max_tokens: 4000
    temperature: 0.7

# Default provider
default: claude
