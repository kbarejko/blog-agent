# AI Provider Configuration
# Configure AI providers for article generation

providers:
  # Claude (Anthropic) - Primary provider
  claude:
    api_key: ${ANTHROPIC_API_KEY}  # Set via environment variable
    model: claude-sonnet-4-20250514
    max_tokens: 4000
    temperature: 1.0

  # OpenAI - GPT-4 Turbo
  openai:
    api_key: ${OPENAI_API_KEY}  # Set via environment variable
    model: gpt-4-turbo
    max_tokens: 4000
    temperature: 0.7

  # OpenAI - GPT-4o (latest and most capable, recommended)
  openai-gpt4o:
    api_key: ${OPENAI_API_KEY}
    model: gpt-4o
    max_tokens: 4000
    temperature: 0.7

  # OpenAI - GPT-4o Mini (faster and cheaper)
  openai-gpt4o-mini:
    api_key: ${OPENAI_API_KEY}
    model: gpt-4o-mini
    max_tokens: 4000
    temperature: 0.7

  # OpenAI - GPT-3.5 Turbo (cheaper, legacy)
  openai-gpt35:
    api_key: ${OPENAI_API_KEY}
    model: gpt-3.5-turbo
    max_tokens: 4000
    temperature: 0.7

  # Google Gemini
  gemini:
    api_key: ${GOOGLE_API_KEY}  # Set via environment variable
    model: gemini-1.5-pro
    max_tokens: 8000
    temperature: 0.9

  # Google Gemini - Flash (faster, cheaper)
  gemini-flash:
    api_key: ${GOOGLE_API_KEY}
    model: gemini-1.5-flash
    max_tokens: 8000
    temperature: 0.9

  # Google Gemini - Pro (legacy)
  gemini-pro:
    api_key: ${GOOGLE_API_KEY}
    model: gemini-pro
    max_tokens: 8000
    temperature: 0.9

  # Ollama - Local models
  ollama:
    model: llama3:latest  # Change to any installed model (llama3, mistral, codellama, etc.)
    host: http://192.168.0.136:11434  # Ollama server URL (Windows host)
    max_tokens: 4000
    temperature: 0.7

  # Ollama - Mistral variant
  ollama-mistral:
    model: mistral:latest
    host: http://192.168.0.136:11434
    max_tokens: 4000
    temperature: 0.7

  # Ollama - CodeLlama variant
  ollama-codellama:
    model: codellama:13b
    host: http://192.168.0.136:11434
    max_tokens: 4000
    temperature: 0.7

# Default provider
default: claude
